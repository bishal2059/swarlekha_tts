{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2234795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969c570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "loaded_tensors = load_file(\"weights/ve.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb73d180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loaded_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d10fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.bias_hh_l0 torch.Size([1024])\n",
      "lstm.bias_hh_l1 torch.Size([1024])\n",
      "lstm.bias_hh_l2 torch.Size([1024])\n",
      "lstm.bias_ih_l0 torch.Size([1024])\n",
      "lstm.bias_ih_l1 torch.Size([1024])\n",
      "lstm.bias_ih_l2 torch.Size([1024])\n",
      "lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "lstm.weight_hh_l1 torch.Size([1024, 256])\n",
      "lstm.weight_hh_l2 torch.Size([1024, 256])\n",
      "lstm.weight_ih_l0 torch.Size([1024, 40])\n",
      "lstm.weight_ih_l1 torch.Size([1024, 256])\n",
      "lstm.weight_ih_l2 torch.Size([1024, 256])\n",
      "proj.bias torch.Size([256])\n",
      "proj.weight torch.Size([256, 256])\n",
      "similarity_bias torch.Size([1])\n",
      "similarity_weight torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for k, v in loaded_tensors.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24344e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tensors[\"lstm.bias_hh_l0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a471cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEModel(torch.nn.Module):\n",
    "    def __init__(self, input_size=40, hidden_size=256, num_layers=3):\n",
    "        super(VEModel, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.proj = torch.nn.Linear(hidden_size,hidden_size)\n",
    "\n",
    "        self.similarity_weight = torch.nn.Parameter(torch.ones(1))\n",
    "        self.similarity_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        out = self.proj(hn[-1])\n",
    "\n",
    "        out = out / torch.norm(out, dim=-1, keepdim=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e8c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_model = VEModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2400a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VEModel(\n",
      "  (lstm): LSTM(40, 256, num_layers=3, batch_first=True)\n",
      "  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(ve_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02c15b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve_model.load_state_dict(loaded_tensors, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deef8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(audio_path, n_mels=40):\n",
    "    \n",
    "    wav, sr = librosa.load(audio_path, sr=16000)\n",
    "    \n",
    "    wav, _ = librosa.effects.trim(wav, top_db=20)\n",
    "    \n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=wav, \n",
    "        sr=sr, \n",
    "        n_fft=1024, \n",
    "        hop_length=256, \n",
    "        win_length=1024, \n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    log_mel = np.log10(np.maximum(mel, 1e-5)).T\n",
    "    \n",
    "    return torch.FloatTensor(log_mel).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16139d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker DNA (256-dim vector): tensor([[-0.0920, -0.0409, -0.0213, -0.0884,  0.0018, -0.0564,  0.0012,  0.0429,\n",
      "         -0.0365, -0.0421,  0.0191, -0.0889, -0.0038, -0.0878, -0.1047, -0.0648,\n",
      "         -0.0010, -0.0142, -0.0020,  0.0093, -0.0525, -0.0171,  0.1558, -0.0247,\n",
      "         -0.0499,  0.0154,  0.0981, -0.0278, -0.0942, -0.1994, -0.0215, -0.0062,\n",
      "          0.0211, -0.1289, -0.0434,  0.0198,  0.0118, -0.0519, -0.0736, -0.0343,\n",
      "         -0.2248,  0.0456,  0.0174, -0.0116,  0.0412, -0.0210, -0.0369,  0.0328,\n",
      "          0.0260, -0.0350, -0.0192, -0.0675, -0.1081, -0.0028, -0.0546, -0.0173,\n",
      "         -0.0017, -0.1323, -0.0889, -0.0315,  0.0080, -0.0858, -0.0128,  0.0197,\n",
      "         -0.0681,  0.0647, -0.1134,  0.0411, -0.0110, -0.0457, -0.0068, -0.0290,\n",
      "         -0.0817,  0.0423,  0.1000,  0.0280,  0.0079,  0.0535,  0.0723, -0.0541,\n",
      "         -0.0459, -0.0193, -0.0040, -0.0851, -0.0476, -0.1243, -0.0868, -0.0052,\n",
      "         -0.0846,  0.0426,  0.0500, -0.0058, -0.0617, -0.0701, -0.0170, -0.0426,\n",
      "          0.0119, -0.0422, -0.0100, -0.1439, -0.0975, -0.0703, -0.0275, -0.0327,\n",
      "          0.0306, -0.0418,  0.0171,  0.0300, -0.0759,  0.0800, -0.1029, -0.0277,\n",
      "         -0.1067,  0.0800,  0.0307, -0.0016, -0.0123,  0.0351, -0.0235,  0.1289,\n",
      "          0.0227,  0.0456,  0.0485, -0.0033, -0.0105, -0.0486,  0.0545, -0.0171,\n",
      "          0.1169, -0.0966, -0.0659,  0.0119,  0.0611, -0.0500, -0.0191, -0.0096,\n",
      "          0.0418, -0.0173, -0.1118, -0.0603, -0.1301, -0.0477, -0.0557, -0.0853,\n",
      "          0.0142, -0.0930,  0.0241,  0.0432,  0.0346,  0.0371, -0.0215, -0.0912,\n",
      "         -0.0528,  0.0931, -0.0728,  0.0567, -0.0524,  0.0093, -0.0432, -0.0542,\n",
      "         -0.0718, -0.0466, -0.0601, -0.0340,  0.0443,  0.0131,  0.0027, -0.0726,\n",
      "          0.0791,  0.0127, -0.0230, -0.0100, -0.1532,  0.0054, -0.0653, -0.0547,\n",
      "         -0.0484,  0.1034, -0.0975, -0.0072,  0.0609,  0.0176,  0.0302,  0.0307,\n",
      "          0.0031, -0.0076, -0.0619, -0.0704, -0.1455,  0.0426, -0.0156,  0.1202,\n",
      "         -0.1738,  0.0777,  0.0039, -0.0628, -0.1319,  0.0637,  0.0071,  0.0543,\n",
      "         -0.0653, -0.0941, -0.0265, -0.0492, -0.0025, -0.0484,  0.0694, -0.0870,\n",
      "         -0.0205, -0.0511, -0.1201, -0.0835,  0.0059,  0.0183,  0.0294, -0.0448,\n",
      "          0.0492,  0.0272, -0.0340, -0.0279,  0.0776,  0.0414,  0.0239, -0.0195,\n",
      "          0.0829, -0.0603,  0.0459,  0.0227, -0.0050, -0.0886,  0.0070, -0.0168,\n",
      "          0.0419, -0.0018, -0.0723,  0.0286,  0.0233, -0.0515,  0.0052, -0.0331,\n",
      "          0.0722, -0.0333, -0.0748, -0.0059,  0.0034,  0.0011, -0.0622, -0.0877,\n",
      "          0.0078, -0.0665, -0.0234, -0.1074, -0.0397, -0.0240,  0.0232, -0.0539]])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = preprocess_audio(\"sample/sample.wav\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedding = ve_model(input_tensor)\n",
    "    \n",
    "embedding = embedding / torch.norm(embedding, dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Speaker DNA (256-dim vector): {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32553e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker embedding extraction model works correctly!\n"
     ]
    }
   ],
   "source": [
    "print(\"Speaker embedding extraction model works correctly!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
