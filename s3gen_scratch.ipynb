{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16014bd-3bb0-49d1-a7ae-8ecceeda58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f358c1b7-4ca0-4d9d-8a26-118981b3c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28cc465-93ff-4d5b-855f-668741a4e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"weights/s3gen.safetensors\"\n",
    "\n",
    "loaded_tensors = load_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f3ca603-7081-478f-91e8-f85fb0ef2afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(loaded_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61f6f359-38cd-486a-b3e1-81f1c61c8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.decoder.estimator.down_blocks.0.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.block1.block.0.weight: torch.Size([256, 320, 3])\n",
      "flow.decoder.estimator.down_blocks.0.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.down_blocks.0.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.down_blocks.0.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.0.res_conv.weight: torch.Size([256, 320, 1])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.down_blocks.0.2.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.final_block.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.final_block.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.final_block.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.final_block.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.final_proj.bias: torch.Size([80])\n",
      "flow.decoder.estimator.final_proj.weight: torch.Size([80, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.0.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.0.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.1.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.1.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.10.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.10.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.11.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.11.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.2.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.2.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.3.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.3.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.4.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.4.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.5.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.5.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.6.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.6.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.7.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.7.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.8.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.8.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block1.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.9.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.0.res_conv.weight: torch.Size([256, 256, 1])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.mid_blocks.9.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.time_mlp.linear_1.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.time_mlp.linear_1.weight: torch.Size([1024, 320])\n",
      "flow.decoder.estimator.time_mlp.linear_2.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.time_mlp.linear_2.weight: torch.Size([1024, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.0.block1.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.block1.block.0.weight: torch.Size([256, 512, 3])\n",
      "flow.decoder.estimator.up_blocks.0.0.block1.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.block1.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.block2.block.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.block2.block.0.weight: torch.Size([256, 256, 3])\n",
      "flow.decoder.estimator.up_blocks.0.0.block2.block.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.block2.block.2.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.mlp.1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.mlp.1.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.0.res_conv.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.0.res_conv.weight: torch.Size([256, 512, 1])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.0.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.1.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.2.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.attn1.to_k.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.attn1.to_out.0.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.attn1.to_out.0.weight: torch.Size([256, 512])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.attn1.to_q.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.attn1.to_v.weight: torch.Size([512, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.ff.net.0.proj.bias: torch.Size([1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.ff.net.0.proj.weight: torch.Size([1024, 256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.ff.net.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.ff.net.2.weight: torch.Size([256, 1024])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.norm1.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.norm1.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.norm3.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.1.3.norm3.weight: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.2.bias: torch.Size([256])\n",
      "flow.decoder.estimator.up_blocks.0.2.weight: torch.Size([256, 256, 3])\n",
      "flow.encoder.after_norm.bias: torch.Size([512])\n",
      "flow.encoder.after_norm.weight: torch.Size([512])\n",
      "flow.encoder.embed.out.0.bias: torch.Size([512])\n",
      "flow.encoder.embed.out.0.weight: torch.Size([512, 512])\n",
      "flow.encoder.embed.out.1.bias: torch.Size([512])\n",
      "flow.encoder.embed.out.1.weight: torch.Size([512])\n",
      "flow.encoder.encoders.0.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.0.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.0.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.0.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.0.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.0.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.0.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.0.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.0.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.0.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.0.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.0.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.0.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.encoders.1.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.1.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.1.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.1.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.1.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.1.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.1.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.1.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.1.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.1.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.1.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.1.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.1.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.encoders.2.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.2.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.2.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.2.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.2.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.2.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.2.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.2.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.2.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.2.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.2.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.2.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.2.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.encoders.3.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.3.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.3.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.3.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.3.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.3.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.3.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.3.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.3.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.3.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.3.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.3.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.3.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.encoders.4.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.4.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.4.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.4.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.4.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.4.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.4.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.4.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.4.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.4.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.4.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.4.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.4.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.encoders.5.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.encoders.5.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.encoders.5.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.encoders.5.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.encoders.5.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.encoders.5.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.5.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.5.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.5.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.5.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.encoders.5.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.encoders.5.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.encoders.5.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.pre_lookahead_layer.conv1.bias: torch.Size([512])\n",
      "flow.encoder.pre_lookahead_layer.conv1.weight: torch.Size([512, 512, 4])\n",
      "flow.encoder.pre_lookahead_layer.conv2.bias: torch.Size([512])\n",
      "flow.encoder.pre_lookahead_layer.conv2.weight: torch.Size([512, 512, 3])\n",
      "flow.encoder.up_embed.out.0.bias: torch.Size([512])\n",
      "flow.encoder.up_embed.out.0.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_embed.out.1.bias: torch.Size([512])\n",
      "flow.encoder.up_embed.out.1.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.up_encoders.0.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.up_encoders.0.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.up_encoders.0.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.0.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.0.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.0.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.1.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.up_encoders.1.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.up_encoders.1.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.up_encoders.1.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.1.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.1.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.1.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.2.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.up_encoders.2.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.up_encoders.2.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.up_encoders.2.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.2.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.2.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.2.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.3.feed_forward.w_1.bias: torch.Size([2048])\n",
      "flow.encoder.up_encoders.3.feed_forward.w_1.weight: torch.Size([2048, 512])\n",
      "flow.encoder.up_encoders.3.feed_forward.w_2.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.feed_forward.w_2.weight: torch.Size([512, 2048])\n",
      "flow.encoder.up_encoders.3.norm_ff.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.norm_ff.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.norm_mha.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.norm_mha.weight: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_k.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_k.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_out.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_out.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_pos.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_q.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_q.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_v.bias: torch.Size([512])\n",
      "flow.encoder.up_encoders.3.self_attn.linear_v.weight: torch.Size([512, 512])\n",
      "flow.encoder.up_encoders.3.self_attn.pos_bias_u: torch.Size([8, 64])\n",
      "flow.encoder.up_encoders.3.self_attn.pos_bias_v: torch.Size([8, 64])\n",
      "flow.encoder.up_layer.conv.bias: torch.Size([512])\n",
      "flow.encoder.up_layer.conv.weight: torch.Size([512, 512, 5])\n",
      "flow.encoder_proj.bias: torch.Size([80])\n",
      "flow.encoder_proj.weight: torch.Size([80, 512])\n",
      "flow.input_embedding.weight: torch.Size([6561, 512])\n",
      "flow.spk_embed_affine_layer.bias: torch.Size([80])\n",
      "flow.spk_embed_affine_layer.weight: torch.Size([80, 192])\n",
      "mel2wav.conv_post.bias: torch.Size([18])\n",
      "mel2wav.conv_post.parametrizations.weight.original0: torch.Size([18, 1, 1])\n",
      "mel2wav.conv_post.parametrizations.weight.original1: torch.Size([18, 64, 7])\n",
      "mel2wav.conv_pre.bias: torch.Size([512])\n",
      "mel2wav.conv_pre.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.conv_pre.parametrizations.weight.original1: torch.Size([512, 80, 7])\n",
      "mel2wav.f0_predictor.classifier.bias: torch.Size([1])\n",
      "mel2wav.f0_predictor.classifier.weight: torch.Size([1, 512])\n",
      "mel2wav.f0_predictor.condnet.0.bias: torch.Size([512])\n",
      "mel2wav.f0_predictor.condnet.0.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.f0_predictor.condnet.0.parametrizations.weight.original1: torch.Size([512, 80, 3])\n",
      "mel2wav.f0_predictor.condnet.2.bias: torch.Size([512])\n",
      "mel2wav.f0_predictor.condnet.2.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.f0_predictor.condnet.2.parametrizations.weight.original1: torch.Size([512, 512, 3])\n",
      "mel2wav.f0_predictor.condnet.4.bias: torch.Size([512])\n",
      "mel2wav.f0_predictor.condnet.4.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.f0_predictor.condnet.4.parametrizations.weight.original1: torch.Size([512, 512, 3])\n",
      "mel2wav.f0_predictor.condnet.6.bias: torch.Size([512])\n",
      "mel2wav.f0_predictor.condnet.6.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.f0_predictor.condnet.6.parametrizations.weight.original1: torch.Size([512, 512, 3])\n",
      "mel2wav.f0_predictor.condnet.8.bias: torch.Size([512])\n",
      "mel2wav.f0_predictor.condnet.8.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.f0_predictor.condnet.8.parametrizations.weight.original1: torch.Size([512, 512, 3])\n",
      "mel2wav.m_source.l_linear.bias: torch.Size([1])\n",
      "mel2wav.m_source.l_linear.weight: torch.Size([1, 9])\n",
      "mel2wav.resblocks.0.activations1.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.activations1.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.activations1.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.activations2.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.activations2.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.activations2.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs1.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs1.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs1.0.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.0.convs1.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs1.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs1.1.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.0.convs1.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs1.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs1.2.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.0.convs2.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs2.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs2.0.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.0.convs2.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs2.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs2.1.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.0.convs2.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.0.convs2.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.0.convs2.2.parametrizations.weight.original1: torch.Size([256, 256, 3])\n",
      "mel2wav.resblocks.1.activations1.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.activations1.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.activations1.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.activations2.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.activations2.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.activations2.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs1.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs1.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs1.0.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.1.convs1.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs1.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs1.1.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.1.convs1.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs1.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs1.2.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.1.convs2.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs2.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs2.0.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.1.convs2.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs2.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs2.1.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.1.convs2.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.1.convs2.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.1.convs2.2.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.resblocks.2.activations1.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.activations1.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.activations1.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.activations2.0.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.activations2.1.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.activations2.2.alpha: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs1.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs1.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs1.0.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.2.convs1.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs1.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs1.1.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.2.convs1.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs1.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs1.2.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.2.convs2.0.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs2.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs2.0.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.2.convs2.1.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs2.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs2.1.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.2.convs2.2.bias: torch.Size([256])\n",
      "mel2wav.resblocks.2.convs2.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.resblocks.2.convs2.2.parametrizations.weight.original1: torch.Size([256, 256, 11])\n",
      "mel2wav.resblocks.3.activations1.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.activations1.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.activations1.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.activations2.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.activations2.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.activations2.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs1.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs1.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs1.0.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.3.convs1.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs1.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs1.1.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.3.convs1.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs1.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs1.2.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.3.convs2.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs2.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs2.0.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.3.convs2.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs2.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs2.1.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.3.convs2.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.3.convs2.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.3.convs2.2.parametrizations.weight.original1: torch.Size([128, 128, 3])\n",
      "mel2wav.resblocks.4.activations1.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.activations1.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.activations1.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.activations2.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.activations2.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.activations2.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs1.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs1.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs1.0.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.4.convs1.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs1.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs1.1.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.4.convs1.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs1.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs1.2.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.4.convs2.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs2.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs2.0.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.4.convs2.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs2.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs2.1.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.4.convs2.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.4.convs2.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.4.convs2.2.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.resblocks.5.activations1.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.activations1.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.activations1.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.activations2.0.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.activations2.1.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.activations2.2.alpha: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs1.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs1.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs1.0.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.5.convs1.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs1.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs1.1.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.5.convs1.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs1.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs1.2.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.5.convs2.0.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs2.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs2.0.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.5.convs2.1.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs2.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs2.1.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.5.convs2.2.bias: torch.Size([128])\n",
      "mel2wav.resblocks.5.convs2.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.resblocks.5.convs2.2.parametrizations.weight.original1: torch.Size([128, 128, 11])\n",
      "mel2wav.resblocks.6.activations1.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.activations1.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.activations1.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.activations2.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.activations2.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.activations2.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs1.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs1.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs1.0.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.6.convs1.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs1.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs1.1.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.6.convs1.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs1.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs1.2.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.6.convs2.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs2.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs2.0.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.6.convs2.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs2.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs2.1.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.6.convs2.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.6.convs2.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.6.convs2.2.parametrizations.weight.original1: torch.Size([64, 64, 3])\n",
      "mel2wav.resblocks.7.activations1.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.activations1.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.activations1.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.activations2.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.activations2.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.activations2.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs1.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs1.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs1.0.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.7.convs1.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs1.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs1.1.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.7.convs1.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs1.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs1.2.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.7.convs2.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs2.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs2.0.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.7.convs2.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs2.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs2.1.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.7.convs2.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.7.convs2.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.7.convs2.2.parametrizations.weight.original1: torch.Size([64, 64, 7])\n",
      "mel2wav.resblocks.8.activations1.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.activations1.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.activations1.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.activations2.0.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.activations2.1.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.activations2.2.alpha: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs1.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs1.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs1.0.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.resblocks.8.convs1.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs1.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs1.1.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.resblocks.8.convs1.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs1.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs1.2.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.resblocks.8.convs2.0.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs2.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs2.0.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.resblocks.8.convs2.1.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs2.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs2.1.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.resblocks.8.convs2.2.bias: torch.Size([64])\n",
      "mel2wav.resblocks.8.convs2.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.resblocks.8.convs2.2.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_downs.0.bias: torch.Size([256])\n",
      "mel2wav.source_downs.0.weight: torch.Size([256, 18, 30])\n",
      "mel2wav.source_downs.1.bias: torch.Size([128])\n",
      "mel2wav.source_downs.1.weight: torch.Size([128, 18, 6])\n",
      "mel2wav.source_downs.2.bias: torch.Size([64])\n",
      "mel2wav.source_downs.2.weight: torch.Size([64, 18, 1])\n",
      "mel2wav.source_resblocks.0.activations1.0.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.activations1.1.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.activations1.2.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.activations2.0.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.activations2.1.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.activations2.2.alpha: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs1.0.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs1.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs1.0.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.0.convs1.1.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs1.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs1.1.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.0.convs1.2.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs1.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs1.2.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.0.convs2.0.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs2.0.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs2.0.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.0.convs2.1.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs2.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs2.1.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.0.convs2.2.bias: torch.Size([256])\n",
      "mel2wav.source_resblocks.0.convs2.2.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.source_resblocks.0.convs2.2.parametrizations.weight.original1: torch.Size([256, 256, 7])\n",
      "mel2wav.source_resblocks.1.activations1.0.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.activations1.1.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.activations1.2.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.activations2.0.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.activations2.1.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.activations2.2.alpha: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs1.0.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs1.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs1.0.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.1.convs1.1.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs1.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs1.1.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.1.convs1.2.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs1.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs1.2.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.1.convs2.0.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs2.0.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs2.0.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.1.convs2.1.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs2.1.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs2.1.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.1.convs2.2.bias: torch.Size([128])\n",
      "mel2wav.source_resblocks.1.convs2.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.source_resblocks.1.convs2.2.parametrizations.weight.original1: torch.Size([128, 128, 7])\n",
      "mel2wav.source_resblocks.2.activations1.0.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.activations1.1.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.activations1.2.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.activations2.0.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.activations2.1.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.activations2.2.alpha: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs1.0.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs1.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs1.0.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_resblocks.2.convs1.1.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs1.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs1.1.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_resblocks.2.convs1.2.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs1.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs1.2.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_resblocks.2.convs2.0.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs2.0.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs2.0.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_resblocks.2.convs2.1.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs2.1.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs2.1.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.source_resblocks.2.convs2.2.bias: torch.Size([64])\n",
      "mel2wav.source_resblocks.2.convs2.2.parametrizations.weight.original0: torch.Size([64, 1, 1])\n",
      "mel2wav.source_resblocks.2.convs2.2.parametrizations.weight.original1: torch.Size([64, 64, 11])\n",
      "mel2wav.ups.0.bias: torch.Size([256])\n",
      "mel2wav.ups.0.parametrizations.weight.original0: torch.Size([512, 1, 1])\n",
      "mel2wav.ups.0.parametrizations.weight.original1: torch.Size([512, 256, 16])\n",
      "mel2wav.ups.1.bias: torch.Size([128])\n",
      "mel2wav.ups.1.parametrizations.weight.original0: torch.Size([256, 1, 1])\n",
      "mel2wav.ups.1.parametrizations.weight.original1: torch.Size([256, 128, 11])\n",
      "mel2wav.ups.2.bias: torch.Size([64])\n",
      "mel2wav.ups.2.parametrizations.weight.original0: torch.Size([128, 1, 1])\n",
      "mel2wav.ups.2.parametrizations.weight.original1: torch.Size([128, 64, 7])\n",
      "speaker_encoder.head.bn1.bias: torch.Size([32])\n",
      "speaker_encoder.head.bn1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.bn1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.bn1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.bn1.weight: torch.Size([32])\n",
      "speaker_encoder.head.bn2.bias: torch.Size([32])\n",
      "speaker_encoder.head.bn2.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.bn2.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.bn2.running_var: torch.Size([32])\n",
      "speaker_encoder.head.bn2.weight: torch.Size([32])\n",
      "speaker_encoder.head.conv1.weight: torch.Size([32, 1, 3, 3])\n",
      "speaker_encoder.head.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer1.0.bn1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer1.0.bn1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn2.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn2.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer1.0.bn2.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn2.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.bn2.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.conv1.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer1.0.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer1.0.shortcut.0.weight: torch.Size([32, 32, 1, 1])\n",
      "speaker_encoder.head.layer1.0.shortcut.1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.shortcut.1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer1.0.shortcut.1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.shortcut.1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer1.0.shortcut.1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer1.1.bn1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn2.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn2.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer1.1.bn2.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn2.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.bn2.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer1.1.conv1.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer1.1.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer2.0.bn1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer2.0.bn1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn2.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn2.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer2.0.bn2.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn2.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.bn2.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.conv1.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer2.0.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer2.0.shortcut.0.weight: torch.Size([32, 32, 1, 1])\n",
      "speaker_encoder.head.layer2.0.shortcut.1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.shortcut.1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer2.0.shortcut.1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.shortcut.1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer2.0.shortcut.1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn1.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn1.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer2.1.bn1.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn1.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn1.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn2.bias: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn2.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.head.layer2.1.bn2.running_mean: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn2.running_var: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.bn2.weight: torch.Size([32])\n",
      "speaker_encoder.head.layer2.1.conv1.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.head.layer2.1.conv2.weight: torch.Size([32, 32, 3, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd1.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd1.linear1.weight: torch.Size([128, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear1.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear1.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear1.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear1.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd1.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd10.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd10.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd10.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd10.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd10.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd10.linear1.weight: torch.Size([128, 416, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear1.batchnorm.bias: torch.Size([416])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear1.batchnorm.running_mean: torch.Size([416])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear1.batchnorm.running_var: torch.Size([416])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear1.batchnorm.weight: torch.Size([416])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd10.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd11.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd11.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd11.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd11.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd11.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd11.linear1.weight: torch.Size([128, 448, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear1.batchnorm.bias: torch.Size([448])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear1.batchnorm.running_mean: torch.Size([448])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear1.batchnorm.running_var: torch.Size([448])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear1.batchnorm.weight: torch.Size([448])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd11.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd12.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd12.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd12.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd12.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd12.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd12.linear1.weight: torch.Size([128, 480, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear1.batchnorm.bias: torch.Size([480])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear1.batchnorm.running_mean: torch.Size([480])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear1.batchnorm.running_var: torch.Size([480])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear1.batchnorm.weight: torch.Size([480])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd12.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd2.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd2.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd2.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd2.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd2.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd2.linear1.weight: torch.Size([128, 160, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear1.batchnorm.bias: torch.Size([160])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear1.batchnorm.running_mean: torch.Size([160])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear1.batchnorm.running_var: torch.Size([160])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear1.batchnorm.weight: torch.Size([160])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd2.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd3.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd3.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd3.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd3.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd3.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd3.linear1.weight: torch.Size([128, 192, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear1.batchnorm.bias: torch.Size([192])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear1.batchnorm.running_mean: torch.Size([192])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear1.batchnorm.running_var: torch.Size([192])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear1.batchnorm.weight: torch.Size([192])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd3.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd4.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd4.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd4.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd4.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd4.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd4.linear1.weight: torch.Size([128, 224, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear1.batchnorm.bias: torch.Size([224])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear1.batchnorm.running_mean: torch.Size([224])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear1.batchnorm.running_var: torch.Size([224])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear1.batchnorm.weight: torch.Size([224])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd4.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd5.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd5.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd5.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd5.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd5.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd5.linear1.weight: torch.Size([128, 256, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear1.batchnorm.bias: torch.Size([256])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear1.batchnorm.running_mean: torch.Size([256])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear1.batchnorm.running_var: torch.Size([256])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear1.batchnorm.weight: torch.Size([256])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd5.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd6.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd6.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd6.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd6.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd6.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd6.linear1.weight: torch.Size([128, 288, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear1.batchnorm.bias: torch.Size([288])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear1.batchnorm.running_mean: torch.Size([288])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear1.batchnorm.running_var: torch.Size([288])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear1.batchnorm.weight: torch.Size([288])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd6.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd7.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd7.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd7.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd7.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd7.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd7.linear1.weight: torch.Size([128, 320, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear1.batchnorm.bias: torch.Size([320])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear1.batchnorm.running_mean: torch.Size([320])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear1.batchnorm.running_var: torch.Size([320])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear1.batchnorm.weight: torch.Size([320])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd7.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd8.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd8.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd8.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd8.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd8.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd8.linear1.weight: torch.Size([128, 352, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear1.batchnorm.bias: torch.Size([352])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear1.batchnorm.running_mean: torch.Size([352])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear1.batchnorm.running_var: torch.Size([352])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear1.batchnorm.weight: torch.Size([352])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd8.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd9.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block1.tdnnd9.linear1.weight: torch.Size([128, 384, 1])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear1.batchnorm.bias: torch.Size([384])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear1.batchnorm.running_mean: torch.Size([384])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear1.batchnorm.running_var: torch.Size([384])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear1.batchnorm.weight: torch.Size([384])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block1.tdnnd9.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd1.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd1.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd1.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd1.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd1.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd1.linear1.weight: torch.Size([128, 256, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear1.batchnorm.bias: torch.Size([256])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear1.batchnorm.running_mean: torch.Size([256])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear1.batchnorm.running_var: torch.Size([256])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear1.batchnorm.weight: torch.Size([256])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd1.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd10.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd10.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd10.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd10.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd10.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd10.linear1.weight: torch.Size([128, 544, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear1.batchnorm.bias: torch.Size([544])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear1.batchnorm.running_mean: torch.Size([544])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear1.batchnorm.running_var: torch.Size([544])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear1.batchnorm.weight: torch.Size([544])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd10.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd11.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd11.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd11.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd11.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd11.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd11.linear1.weight: torch.Size([128, 576, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear1.batchnorm.bias: torch.Size([576])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear1.batchnorm.running_mean: torch.Size([576])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear1.batchnorm.running_var: torch.Size([576])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear1.batchnorm.weight: torch.Size([576])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd11.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd12.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd12.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd12.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd12.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd12.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd12.linear1.weight: torch.Size([128, 608, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear1.batchnorm.bias: torch.Size([608])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear1.batchnorm.running_mean: torch.Size([608])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear1.batchnorm.running_var: torch.Size([608])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear1.batchnorm.weight: torch.Size([608])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd12.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd13.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd13.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd13.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd13.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd13.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd13.linear1.weight: torch.Size([128, 640, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear1.batchnorm.bias: torch.Size([640])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear1.batchnorm.running_mean: torch.Size([640])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear1.batchnorm.running_var: torch.Size([640])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear1.batchnorm.weight: torch.Size([640])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd13.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd14.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd14.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd14.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd14.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd14.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd14.linear1.weight: torch.Size([128, 672, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.bias: torch.Size([672])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.running_mean: torch.Size([672])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.running_var: torch.Size([672])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear1.batchnorm.weight: torch.Size([672])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd14.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd15.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd15.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd15.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd15.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd15.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd15.linear1.weight: torch.Size([128, 704, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear1.batchnorm.bias: torch.Size([704])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear1.batchnorm.running_mean: torch.Size([704])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear1.batchnorm.running_var: torch.Size([704])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear1.batchnorm.weight: torch.Size([704])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd15.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd16.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd16.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd16.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd16.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd16.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd16.linear1.weight: torch.Size([128, 736, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear1.batchnorm.bias: torch.Size([736])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear1.batchnorm.running_mean: torch.Size([736])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear1.batchnorm.running_var: torch.Size([736])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear1.batchnorm.weight: torch.Size([736])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd16.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd17.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd17.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd17.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd17.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd17.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd17.linear1.weight: torch.Size([128, 768, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear1.batchnorm.bias: torch.Size([768])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear1.batchnorm.running_mean: torch.Size([768])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear1.batchnorm.running_var: torch.Size([768])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear1.batchnorm.weight: torch.Size([768])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd17.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd18.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd18.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd18.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd18.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd18.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd18.linear1.weight: torch.Size([128, 800, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear1.batchnorm.bias: torch.Size([800])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear1.batchnorm.running_mean: torch.Size([800])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear1.batchnorm.running_var: torch.Size([800])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear1.batchnorm.weight: torch.Size([800])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd18.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd19.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd19.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd19.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd19.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd19.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd19.linear1.weight: torch.Size([128, 832, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear1.batchnorm.bias: torch.Size([832])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear1.batchnorm.running_mean: torch.Size([832])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear1.batchnorm.running_var: torch.Size([832])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear1.batchnorm.weight: torch.Size([832])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd19.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd2.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd2.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd2.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd2.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd2.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd2.linear1.weight: torch.Size([128, 288, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear1.batchnorm.bias: torch.Size([288])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear1.batchnorm.running_mean: torch.Size([288])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear1.batchnorm.running_var: torch.Size([288])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear1.batchnorm.weight: torch.Size([288])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd2.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd20.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd20.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd20.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd20.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd20.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd20.linear1.weight: torch.Size([128, 864, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear1.batchnorm.bias: torch.Size([864])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear1.batchnorm.running_mean: torch.Size([864])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear1.batchnorm.running_var: torch.Size([864])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear1.batchnorm.weight: torch.Size([864])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd20.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd21.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd21.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd21.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd21.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd21.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd21.linear1.weight: torch.Size([128, 896, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear1.batchnorm.bias: torch.Size([896])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear1.batchnorm.running_mean: torch.Size([896])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear1.batchnorm.running_var: torch.Size([896])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear1.batchnorm.weight: torch.Size([896])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd21.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd22.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd22.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd22.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd22.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd22.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd22.linear1.weight: torch.Size([128, 928, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear1.batchnorm.bias: torch.Size([928])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear1.batchnorm.running_mean: torch.Size([928])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear1.batchnorm.running_var: torch.Size([928])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear1.batchnorm.weight: torch.Size([928])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd22.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd23.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd23.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd23.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd23.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd23.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd23.linear1.weight: torch.Size([128, 960, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear1.batchnorm.bias: torch.Size([960])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear1.batchnorm.running_mean: torch.Size([960])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear1.batchnorm.running_var: torch.Size([960])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear1.batchnorm.weight: torch.Size([960])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd23.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd24.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd24.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd24.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd24.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd24.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd24.linear1.weight: torch.Size([128, 992, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear1.batchnorm.bias: torch.Size([992])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear1.batchnorm.running_mean: torch.Size([992])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear1.batchnorm.running_var: torch.Size([992])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear1.batchnorm.weight: torch.Size([992])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd24.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd3.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd3.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd3.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd3.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd3.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd3.linear1.weight: torch.Size([128, 320, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear1.batchnorm.bias: torch.Size([320])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear1.batchnorm.running_mean: torch.Size([320])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear1.batchnorm.running_var: torch.Size([320])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear1.batchnorm.weight: torch.Size([320])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd3.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd4.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd4.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd4.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd4.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd4.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd4.linear1.weight: torch.Size([128, 352, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear1.batchnorm.bias: torch.Size([352])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear1.batchnorm.running_mean: torch.Size([352])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear1.batchnorm.running_var: torch.Size([352])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear1.batchnorm.weight: torch.Size([352])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd4.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd5.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd5.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd5.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd5.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd5.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd5.linear1.weight: torch.Size([128, 384, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear1.batchnorm.bias: torch.Size([384])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear1.batchnorm.running_mean: torch.Size([384])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear1.batchnorm.running_var: torch.Size([384])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear1.batchnorm.weight: torch.Size([384])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd5.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd6.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd6.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd6.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd6.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd6.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd6.linear1.weight: torch.Size([128, 416, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear1.batchnorm.bias: torch.Size([416])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear1.batchnorm.running_mean: torch.Size([416])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear1.batchnorm.running_var: torch.Size([416])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear1.batchnorm.weight: torch.Size([416])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd6.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd7.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd7.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd7.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd7.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd7.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd7.linear1.weight: torch.Size([128, 448, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear1.batchnorm.bias: torch.Size([448])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear1.batchnorm.running_mean: torch.Size([448])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear1.batchnorm.running_var: torch.Size([448])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear1.batchnorm.weight: torch.Size([448])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd7.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd8.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd8.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd8.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd8.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd8.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd8.linear1.weight: torch.Size([128, 480, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear1.batchnorm.bias: torch.Size([480])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear1.batchnorm.running_mean: torch.Size([480])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear1.batchnorm.running_var: torch.Size([480])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear1.batchnorm.weight: torch.Size([480])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd8.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd9.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block2.tdnnd9.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd9.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block2.tdnnd9.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd9.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block2.tdnnd9.linear1.weight: torch.Size([128, 512, 1])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear1.batchnorm.bias: torch.Size([512])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear1.batchnorm.running_mean: torch.Size([512])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear1.batchnorm.running_var: torch.Size([512])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear1.batchnorm.weight: torch.Size([512])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block2.tdnnd9.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd1.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd1.linear1.weight: torch.Size([128, 512, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear1.batchnorm.bias: torch.Size([512])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear1.batchnorm.running_mean: torch.Size([512])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear1.batchnorm.running_var: torch.Size([512])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear1.batchnorm.weight: torch.Size([512])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd1.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd10.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd10.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd10.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd10.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd10.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd10.linear1.weight: torch.Size([128, 800, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear1.batchnorm.bias: torch.Size([800])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear1.batchnorm.running_mean: torch.Size([800])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear1.batchnorm.running_var: torch.Size([800])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear1.batchnorm.weight: torch.Size([800])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd10.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd11.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd11.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd11.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd11.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd11.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd11.linear1.weight: torch.Size([128, 832, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear1.batchnorm.bias: torch.Size([832])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear1.batchnorm.running_mean: torch.Size([832])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear1.batchnorm.running_var: torch.Size([832])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear1.batchnorm.weight: torch.Size([832])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd11.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd12.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd12.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd12.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd12.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd12.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd12.linear1.weight: torch.Size([128, 864, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear1.batchnorm.bias: torch.Size([864])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear1.batchnorm.running_mean: torch.Size([864])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear1.batchnorm.running_var: torch.Size([864])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear1.batchnorm.weight: torch.Size([864])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd12.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd13.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd13.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd13.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd13.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd13.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd13.linear1.weight: torch.Size([128, 896, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear1.batchnorm.bias: torch.Size([896])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear1.batchnorm.running_mean: torch.Size([896])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear1.batchnorm.running_var: torch.Size([896])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear1.batchnorm.weight: torch.Size([896])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd13.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd14.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd14.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd14.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd14.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd14.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd14.linear1.weight: torch.Size([128, 928, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear1.batchnorm.bias: torch.Size([928])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear1.batchnorm.running_mean: torch.Size([928])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear1.batchnorm.running_var: torch.Size([928])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear1.batchnorm.weight: torch.Size([928])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd14.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd15.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd15.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd15.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd15.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd15.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd15.linear1.weight: torch.Size([128, 960, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.bias: torch.Size([960])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.running_mean: torch.Size([960])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.running_var: torch.Size([960])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear1.batchnorm.weight: torch.Size([960])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd15.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd16.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd16.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd16.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd16.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd16.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd16.linear1.weight: torch.Size([128, 992, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear1.batchnorm.bias: torch.Size([992])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear1.batchnorm.running_mean: torch.Size([992])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear1.batchnorm.running_var: torch.Size([992])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear1.batchnorm.weight: torch.Size([992])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd16.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd2.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd2.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd2.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd2.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd2.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd2.linear1.weight: torch.Size([128, 544, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear1.batchnorm.bias: torch.Size([544])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear1.batchnorm.running_mean: torch.Size([544])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear1.batchnorm.running_var: torch.Size([544])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear1.batchnorm.weight: torch.Size([544])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd2.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd3.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd3.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd3.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd3.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd3.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd3.linear1.weight: torch.Size([128, 576, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear1.batchnorm.bias: torch.Size([576])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear1.batchnorm.running_mean: torch.Size([576])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear1.batchnorm.running_var: torch.Size([576])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear1.batchnorm.weight: torch.Size([576])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd3.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd4.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd4.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd4.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd4.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd4.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd4.linear1.weight: torch.Size([128, 608, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear1.batchnorm.bias: torch.Size([608])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear1.batchnorm.running_mean: torch.Size([608])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear1.batchnorm.running_var: torch.Size([608])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear1.batchnorm.weight: torch.Size([608])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd4.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd5.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd5.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd5.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd5.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd5.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd5.linear1.weight: torch.Size([128, 640, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear1.batchnorm.bias: torch.Size([640])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear1.batchnorm.running_mean: torch.Size([640])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear1.batchnorm.running_var: torch.Size([640])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear1.batchnorm.weight: torch.Size([640])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd5.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd6.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd6.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd6.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd6.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd6.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd6.linear1.weight: torch.Size([128, 672, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear1.batchnorm.bias: torch.Size([672])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear1.batchnorm.running_mean: torch.Size([672])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear1.batchnorm.running_var: torch.Size([672])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear1.batchnorm.weight: torch.Size([672])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd6.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd7.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd7.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd7.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd7.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd7.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd7.linear1.weight: torch.Size([128, 704, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear1.batchnorm.bias: torch.Size([704])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear1.batchnorm.running_mean: torch.Size([704])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear1.batchnorm.running_var: torch.Size([704])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear1.batchnorm.weight: torch.Size([704])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd7.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd8.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd8.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd8.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd8.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd8.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd8.linear1.weight: torch.Size([128, 736, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear1.batchnorm.bias: torch.Size([736])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear1.batchnorm.running_mean: torch.Size([736])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear1.batchnorm.running_var: torch.Size([736])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear1.batchnorm.weight: torch.Size([736])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd8.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd9.cam_layer.linear1.bias: torch.Size([64])\n",
      "speaker_encoder.xvector.block3.tdnnd9.cam_layer.linear1.weight: torch.Size([64, 128, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd9.cam_layer.linear2.bias: torch.Size([32])\n",
      "speaker_encoder.xvector.block3.tdnnd9.cam_layer.linear2.weight: torch.Size([32, 64, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd9.cam_layer.linear_local.weight: torch.Size([32, 128, 3])\n",
      "speaker_encoder.xvector.block3.tdnnd9.linear1.weight: torch.Size([128, 768, 1])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear1.batchnorm.bias: torch.Size([768])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear1.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear1.batchnorm.running_mean: torch.Size([768])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear1.batchnorm.running_var: torch.Size([768])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear1.batchnorm.weight: torch.Size([768])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear2.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear2.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear2.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear2.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.block3.tdnnd9.nonlinear2.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.dense.linear.weight: torch.Size([192, 1024, 1])\n",
      "speaker_encoder.xvector.dense.nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.dense.nonlinear.batchnorm.running_mean: torch.Size([192])\n",
      "speaker_encoder.xvector.dense.nonlinear.batchnorm.running_var: torch.Size([192])\n",
      "speaker_encoder.xvector.out_nonlinear.batchnorm.bias: torch.Size([512])\n",
      "speaker_encoder.xvector.out_nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.out_nonlinear.batchnorm.running_mean: torch.Size([512])\n",
      "speaker_encoder.xvector.out_nonlinear.batchnorm.running_var: torch.Size([512])\n",
      "speaker_encoder.xvector.out_nonlinear.batchnorm.weight: torch.Size([512])\n",
      "speaker_encoder.xvector.tdnn.linear.weight: torch.Size([128, 320, 5])\n",
      "speaker_encoder.xvector.tdnn.nonlinear.batchnorm.bias: torch.Size([128])\n",
      "speaker_encoder.xvector.tdnn.nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.tdnn.nonlinear.batchnorm.running_mean: torch.Size([128])\n",
      "speaker_encoder.xvector.tdnn.nonlinear.batchnorm.running_var: torch.Size([128])\n",
      "speaker_encoder.xvector.tdnn.nonlinear.batchnorm.weight: torch.Size([128])\n",
      "speaker_encoder.xvector.transit1.linear.weight: torch.Size([256, 512, 1])\n",
      "speaker_encoder.xvector.transit1.nonlinear.batchnorm.bias: torch.Size([512])\n",
      "speaker_encoder.xvector.transit1.nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.transit1.nonlinear.batchnorm.running_mean: torch.Size([512])\n",
      "speaker_encoder.xvector.transit1.nonlinear.batchnorm.running_var: torch.Size([512])\n",
      "speaker_encoder.xvector.transit1.nonlinear.batchnorm.weight: torch.Size([512])\n",
      "speaker_encoder.xvector.transit2.linear.weight: torch.Size([512, 1024, 1])\n",
      "speaker_encoder.xvector.transit2.nonlinear.batchnorm.bias: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit2.nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.transit2.nonlinear.batchnorm.running_mean: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit2.nonlinear.batchnorm.running_var: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit2.nonlinear.batchnorm.weight: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit3.linear.weight: torch.Size([512, 1024, 1])\n",
      "speaker_encoder.xvector.transit3.nonlinear.batchnorm.bias: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit3.nonlinear.batchnorm.num_batches_tracked: torch.Size([])\n",
      "speaker_encoder.xvector.transit3.nonlinear.batchnorm.running_mean: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit3.nonlinear.batchnorm.running_var: torch.Size([1024])\n",
      "speaker_encoder.xvector.transit3.nonlinear.batchnorm.weight: torch.Size([1024])\n",
      "tokenizer._mel_filters: torch.Size([128, 201])\n",
      "tokenizer.encoder.blocks.0.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.0.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.0.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.0.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.0.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.0.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.0.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.0.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.0.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.0.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.1.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.1.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.1.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.1.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.1.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.1.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.1.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.1.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.1.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.2.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.2.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.2.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.2.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.2.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.2.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.2.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.2.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.2.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.3.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.3.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.3.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.3.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.3.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.3.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.3.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.3.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.3.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.4.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.4.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.4.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.4.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.4.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.4.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.4.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.4.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.4.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.attn.fsmn_block.weight: torch.Size([1280, 1, 31])\n",
      "tokenizer.encoder.blocks.5.attn.key.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.5.attn.out.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.attn.out.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.5.attn.query.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.attn.query.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.5.attn.value.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.attn.value.weight: torch.Size([1280, 1280])\n",
      "tokenizer.encoder.blocks.5.attn_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.attn_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.mlp.0.bias: torch.Size([5120])\n",
      "tokenizer.encoder.blocks.5.mlp.0.weight: torch.Size([5120, 1280])\n",
      "tokenizer.encoder.blocks.5.mlp.2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.mlp.2.weight: torch.Size([1280, 5120])\n",
      "tokenizer.encoder.blocks.5.mlp_ln.bias: torch.Size([1280])\n",
      "tokenizer.encoder.blocks.5.mlp_ln.weight: torch.Size([1280])\n",
      "tokenizer.encoder.conv1.bias: torch.Size([1280])\n",
      "tokenizer.encoder.conv1.weight: torch.Size([1280, 128, 3])\n",
      "tokenizer.encoder.conv2.bias: torch.Size([1280])\n",
      "tokenizer.encoder.conv2.weight: torch.Size([1280, 1280, 3])\n",
      "tokenizer.quantizer._codebook.project_down.bias: torch.Size([8])\n",
      "tokenizer.quantizer._codebook.project_down.weight: torch.Size([8, 1280])\n"
     ]
    }
   ],
   "source": [
    "for k, v in loaded_tensors.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
